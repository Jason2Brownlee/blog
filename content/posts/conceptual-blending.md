---
date: '2025-01-23T09:07:26+11:00'
title: 'Conceptual Blending and LLMs'
---

I [tripped over](https://x.com/poetengineer__/status/1882164884227736012) the concept of "[Conceptual Blending](https://en.wikipedia.org/wiki/Conceptual_blending)" (thanks Kat).

From wikipedia:

> According to this theory, elements and vital relations from diverse scenarios are "blended" in a subconscious process, which is assumed to be ubiquitous to everyday thought and language.

Again related back to ensembles/multiple perspectives, connected to the [bag of analogies](/blog/posts/abstraction-and-analogies/) thinking from the other day.

The best presentation of the idea is the book "[The Way We Think: Conceptual Blending and The Mind's Hidden Complexities](https://www.goodreads.com/book/show/799328.The_Way_We_Think)" by Gilles Fauconnier and Mark Turner.

From the preface:

> Almost invisibly to consciousness, conceptual blending choreographs vast networks of conceptual meaning, yielding cognitive products that, at the conscious level, appear simple. The way we think is not the way we think we think. Everyday thought seems straightforward, but even our simplest thinking is astonishingly complex.

From our our claude friend:

---

Conceptual blending is a cognitive theory that explains how humans combine different mental concepts or "spaces" to create new meanings and understandings. Think of it like a mental mixing bowl where we combine different ideas to create something new.

Here's a simple example: When we understand the phrase "computer virus," we're blending two distinct concepts:
1. Our knowledge of biological viruses (they spread, infect, replicate)
2. Our knowledge of computer programs

By blending these, we create a new concept that helps us understand how malicious software spreads and "infects" computers, even though there's nothing literally biological about it.

Another everyday example is when we talk about "digging deeper" into a topic. We're blending:
- The physical act of digging
- The abstract process of understanding

This creates a powerful metaphor that helps us conceptualize how understanding works.

The theory was developed by Gilles Fauconnier and Mark Turner in the 1990s, and it helps explain how humans are able to be creative, understand metaphors, and make novel connections between different domains of knowledge.

---

Nod.

Can it help us with thinking about LLM reasoning/thinking?

Probably.

Here's out gpt4o friend (edited by me):

---
Conceptual blending involves combining elements from multiple mental spaces (structured sets of knowledge) to produce novel meanings or ideas, which aligns with the way LLMs generate outputs by synthesizing patterns in data.

Here’s how conceptual blending can help us understand LLM reasoning:

### 1. **Integration of Multiple Sources (Mental Spaces)**
   - **Human Cognitive Analogy:** In conceptual blending, humans combine information from different mental spaces to form new, coherent ideas. For instance, blending "love" with "journey" creates metaphors like "our relationship is at a crossroads."
   - **LLM Parallels:** LLMs combine semantic patterns and associations from vast datasets, effectively blending contexts (e.g., words, phrases, or concepts) to generate outputs. For example, when asked to create a metaphor, an LLM retrieves patterns of how concepts have been blended in language and applies those patterns in new combinations.

### 2. **Dynamic Meaning Construction**
   - **Human Cognitive Analogy:** Conceptual blending explains how people dynamically construct meanings based on context, adapting ideas creatively to fit new scenarios.
   - **LLM Parallels:** LLMs dynamically generate responses by predicting the next word or sequence based on probabilistic relationships, effectively simulating the creation of blended mental spaces. This allows them to respond flexibly to prompts that require nuanced understanding or creativity.

### 3. **Cross-Domain Reasoning**
   - **Human Cognitive Analogy:** Conceptual blending facilitates reasoning across domains by mapping structures from one domain (e.g., mathematics) onto another (e.g., storytelling), enabling innovation.
   - **LLM Parallels:** LLMs exhibit cross-domain reasoning when they produce analogies, metaphors, or problem-solving suggestions by identifying shared structures or relationships across different fields of knowledge.

### 4. **Emergent Creativity**
   - **Human Cognitive Analogy:** Blends often result in emergent properties not present in the original inputs (e.g., combining "ship" and "space" to create the concept of a "spaceship").
   - **LLM Parallels:** LLMs can generate novel responses by combining linguistic patterns in ways that produce new, creative ideas, though not with intentionality. This emergent behavior mimics how humans create new meanings through blending.

### 5. **Lack of Intentionality vs. Simulation of Intent**
   - **Human Cognitive Analogy:** Human conceptual blending involves intentional, goal-directed thinking, often with emotional or contextual relevance.
   - **LLM Parallels:** While LLMs do not have intentionality, their ability to generate coherent blends of concepts can simulate reasoning processes, making them effective tools for tasks requiring apparent "thought."

### Limitations in the Analogy:
- **Absent Consciousness:** Unlike humans, LLMs lack subjective experience or awareness of the blends they generate.
- **No Intrinsic Motivation:** LLMs operate solely based on patterns in their training data, without goals or personal agency in constructing conceptual blends.

### Conclusion:
Conceptual blending provides a useful lens for understanding LLMs because both processes involve synthesizing multiple inputs into coherent outputs. While LLMs do not "think" in the human sense, their capacity to combine and generate patterns of meaning mirrors key aspects of how humans construct and extend knowledge creatively. This perspective highlights the sophistication of LLM outputs and their alignment with principles of cognitive linguistics.

---

I like it.

I always dislike it when people say LLMs cannot invent or come up with anything new.

Of course they can, and in a way that most of us do all the time.

I'm no philosopher or expert in epistemology, but:

* They can combine disparate ideas.
* They can also interpolate in idea space.

Both sound like coming up with new stuff to me.

Reminds me of _idea sex_ in Matt Ridley's book "[The Rational Optimist](https://www.goodreads.com/book/show/7776209-the-rational-optimist)".

Quote from the beginning of the book:

> “The answer, I believe, is that at some point in human history, ideas began to meet and mate, to have sex with each other.”

A snippet from gpt4o:

> Key Points of "Idea Sex":
> 1. Collaboration Drives Progress: Ridley argues that human progress stems not just from individual ingenuity but from the ability to exchange and recombine ideas with others. This "intercourse" of ideas leads to innovations that no single person or culture could achieve in isolation.
> 2. Analogy with Genetic Mixing: Just as biological reproduction combines genes to create new combinations and drive evolution, exchanging ideas creates new intellectual "offspring," fostering creativity and improvement.
> 3. Cultural and Economic Exchange: Ridley highlights the importance of trade, specialization, and communication as mechanisms for spreading and blending ideas across disciplines, cultures, and regions. This process has fueled human advancement throughout history.
> 4. Networks of Innovation: The metaphor of "idea sex" is closely linked to the concept of networks. When people connect and share knowledge—whether through conversation, technology, or markets—they create an ecosystem where ideas can interact, mutate, and evolve.

Idea sex is across minds of course, whereas conceptual blending is across ideas/idea spaces within a mind (or LLM model?).

Good stuff.

I forced gpt4o to integrate the two ideas. Not that exciting, I'll spare you.
